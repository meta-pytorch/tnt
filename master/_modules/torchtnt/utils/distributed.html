


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchtnt.utils.distributed &mdash; TorchTNT master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

  
  
    <div id="redirect-banner" style="display: none">
      <p>
        🎉 This is the public documentation. There is internal documentation for Meta employees at
        <a href="https://www.internalfb.com/intern/staticdocs/torchtnt/">https://www.internalfb.com/intern/staticdocs/torchtnt/</a>
      </p>
    </div>
  

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (unstable)
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpointing.html">Checkpointing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/unit.html">Unit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/auto_unit.html">AutoUnit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/eval.html">Evaluate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/predict.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/fit.html">Fit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/state.html">State</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/callbacks.html">Callbacks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../utils/utils.html">Utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchtnt.utils.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchtnt.utils.distributed</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="c1"># pyre-strict</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span><span class="p">,</span> <span class="n">multiprocessing</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.elastic.utils.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_free_port</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">ParamSpec</span>


<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="n">DistObjList</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">]]</span>
<span class="n">TParams</span> <span class="o">=</span> <span class="n">ParamSpec</span><span class="p">(</span><span class="s2">&quot;TParams&quot;</span><span class="p">)</span>
<span class="n">TReturn</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;TReturn&quot;</span><span class="p">)</span>

<span class="n">logger</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="PGWrapper"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.PGWrapper.html#torchtnt.utils.distributed.PGWrapper">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">PGWrapper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper around ProcessGroup that allows collectives to be issued in a</span>
<span class="sd">    consistent fashion regardless of the following scenarios:</span>

<span class="sd">        pg is None, distributed is initialized:     use WORLD as pg</span>
<span class="sd">        pg is None, distributed is not initialized: single process app</span>
<span class="sd">        pg is not None:                             use pg</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="PGWrapper.__init__"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.PGWrapper.html#torchtnt.utils.distributed.PGWrapper.__init__">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">barrier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">broadcast_object_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_list</span><span class="p">:</span> <span class="n">DistObjList</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">obj_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">all_gather_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_list</span><span class="p">:</span> <span class="n">DistObjList</span><span class="p">,</span> <span class="n">obj</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obj_list</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="n">obj_list</span><span class="p">)</span>  <span class="c1"># to make pyre happy</span>
            <span class="n">obj_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
            <span class="k">return</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_object</span><span class="p">(</span><span class="n">obj_list</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">scatter_object_list</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">output_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span>
        <span class="n">input_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DistObjList</span><span class="p">],</span>
        <span class="n">src</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The src rank&#39;s input_list for scatter_object_list must not be None.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The length of input_list </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> for scatter_object_list &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;must be the same as the process group&#39;s world size (</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">).&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">world_size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span>

        <span class="c1"># scatter_object_list does not yet support NCCL backend</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">obj_list</span><span class="o">=</span><span class="n">input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">)</span>
            <span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
            <span class="k">return</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">scatter_object_list</span><span class="p">(</span><span class="n">output_list</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_global_rank"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_global_rank.html#torchtnt.utils.distributed.get_global_rank">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_global_rank</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get rank using torch.distributed if available. Otherwise, the RANK env var instead if initialized.</span>
<span class="sd">    Returns 0 if neither condition is met.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

    <span class="n">environ_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_rank</span><span class="o">.</span><span class="n">isdecimal</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="get_local_rank"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_local_rank.html#torchtnt.utils.distributed.get_local_rank">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get rank using the ``LOCAL_RANK`` environment variable, if populated: https://pytorch.org/docs/stable/elastic/run.html#environment-variables</span>
<span class="sd">    Defaults to 0 if ``LOCAL_RANK`` is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">environ_local_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_local_rank</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">environ_local_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">get_local_world_size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get local world size using the ``LOCAL_WORLD_SIZE`` environment variable, if populated: https://pytorch.org/docs/stable/elastic/run.html#environment-variables</span>
<span class="sd">    Defaults to 1 if ``LOCAL_WORLD_SIZE`` is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">environ_local_world_size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_WORLD_SIZE&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_local_world_size</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">environ_local_world_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_world_size.html#torchtnt.utils.distributed.get_world_size">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_world_size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get world size using torch.distributed if available. Otherwise, the WORLD_SIZE env var is used instead if initialized.</span>
<span class="sd">    Returns 1 if neither condition is met.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="n">world_size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">world_size</span><span class="o">.</span><span class="n">isdecimal</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="mi">1</span></div>


<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.barrier.html#torchtnt.utils.distributed.barrier">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">barrier</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a synchronization point across all processes when using distributed.</span>
<span class="sd">    If torch.distributed is initialized, this function will invoke a barrier across the global process group.</span>
<span class="sd">    For more granular process group wrapping, please refer to :class:`~torchtnt.utils.PGWrapper`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()):</span>
        <span class="k">return</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span></div>


<div class="viewcode-block" id="destroy_process_group"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.destroy_process_group.html#torchtnt.utils.distributed.destroy_process_group">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">destroy_process_group</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Destroy the global process group, if one is already initialized.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span></div>


<div class="viewcode-block" id="get_process_group_backend_from_device"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_process_group_backend_from_device.html#torchtnt.utils.distributed.get_process_group_backend_from_device">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_process_group_backend_from_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function that gets the default process group backend from the device.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;nccl&quot;</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="s2">&quot;gloo&quot;</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid world_size value provided: </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">. Value must be greater than 0.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid rank value provided: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">. Value must be greater than non-negative.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="n">world_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid rank and world_size values provided: rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">, world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">. Rank must be less than world_size.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="get_file_init_method"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_file_init_method.html#torchtnt.utils.distributed.get_file_init_method">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_file_init_method</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets init method for the TCP protocol for the distributed environment.</span>
<span class="sd">    For more information, see here: https://pytorch.org/docs/stable/distributed.html#shared-file-system-initialization</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: global number of workers. If ``None``, the default is fetched using :function:`get_world_size`.</span>
<span class="sd">        rank: Global rank of the worker calling the function. If ``None``, the default is fetched using :function:`get_global_rank`.</span>
<span class="sd">        filename: The filename to use for synchronization. If ``None``, a new temporary file is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span> <span class="k">if</span> <span class="n">world_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_global_rank</span><span class="p">()</span>
    <span class="n">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="o">.</span><span class="n">name</span>
    <span class="n">init_method</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;file://</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">?world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&amp;rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">init_method</span></div>


<div class="viewcode-block" id="get_tcp_init_method"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_tcp_init_method.html#torchtnt.utils.distributed.get_tcp_init_method">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_tcp_init_method</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hostname</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">port</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets init method for the TCP protocol for the distributed environment.</span>
<span class="sd">    For more information, see here: https://pytorch.org/docs/stable/distributed.html#tcp-initialization.</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: global number of workers. If ``None``, the default is fetched using :function:`get_world_size`.</span>
<span class="sd">        rank: Global rank of the worker calling the function. If ``None``, the default is fetched using :function:`get_global_rank`.</span>
<span class="sd">        hostname: an address that belongs to the rank 0 process. If ``None``, then ``localhost`` is used.</span>
<span class="sd">        port: A free port to use for communication. If ``None``, this port is automatically selected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span> <span class="k">if</span> <span class="n">world_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_global_rank</span><span class="p">()</span>
    <span class="n">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">host_addr</span> <span class="o">=</span> <span class="n">hostname</span> <span class="k">if</span> <span class="n">hostname</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">host_port</span> <span class="o">=</span> <span class="n">port</span> <span class="k">if</span> <span class="n">port</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_free_port</span><span class="p">()</span>
    <span class="n">init_method</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;tcp://</span><span class="si">{</span><span class="n">host_addr</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">host_port</span><span class="si">}</span><span class="s2">?world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&amp;rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">init_method</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_simple_all_gather_tensors</span><span class="p">(</span>
    <span class="n">result</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">],</span> <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">stacked_result_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">gathered_result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">stacked_result_sizes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gathered_result</span>


<div class="viewcode-block" id="all_gather_tensors"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.all_gather_tensors.html#torchtnt.utils.distributed.all_gather_tensors">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">all_gather_tensors</span><span class="p">(</span>
    <span class="n">result</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function to gather tensors from several distributed processes onto a list that is broadcasted to all processes.</span>
<span class="sd">    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case</span>
<span class="sd">    tensors are padded, gathered and then trimmed to secure equal workload for all processes.</span>

<span class="sd">    Args:</span>
<span class="sd">        result: the value to sync</span>
<span class="sd">        group: the process group to gather results from. Defaults to all processes (world)</span>

<span class="sd">    Return:</span>
<span class="sd">        gathered_result: list with size equal to the process group where</span>
<span class="sd">            gathered_result[i] corresponds to result tensor from process i</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if torch.distributed is not available or not initialized</span>
    <span class="c1"># return single-item list containing the result</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">result</span><span class="p">]</span>

    <span class="c1"># convert tensors to contiguous format</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># if the tensor is scalar, things are easy</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_simple_all_gather_tensors</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># gather sizes of all tensors</span>
    <span class="n">local_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">stacked_local_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">local_size</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">local_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">stacked_local_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">local_size</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">local_size</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># if the backend is NCCL, we can gather the differently sized tensors without padding</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
        <span class="n">gathered_result</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="n">size</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">local_sizes</span><span class="p">]</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gathered_result</span>

    <span class="c1"># if shapes are all the same, then do a simple gather:</span>
    <span class="n">stacked_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">)</span>
    <span class="n">max_size</span> <span class="o">=</span> <span class="n">stacked_sizes</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">min_size</span> <span class="o">=</span> <span class="n">stacked_sizes</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">all_sizes_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">max_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">all_sizes_equal</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_simple_all_gather_tensors</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># if not, we need to pad each local tensor to maximum size, gather and then truncate</span>
    <span class="n">pad_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pad_by</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_size</span> <span class="o">-</span> <span class="n">local_size</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">pad_by</span><span class="p">):</span>
        <span class="n">pad_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pad_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">result_padded</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">pad_dims</span><span class="p">)</span>
    <span class="n">stacked_result_padded</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">result_padded</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">gathered_result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">stacked_result_padded</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">result_padded</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">result_padded</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result_padded</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">item_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">):</span>
        <span class="n">slice_param</span> <span class="o">=</span> <span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">item_size</span><span class="p">]</span>
        <span class="n">gathered_result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">gathered_result</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">slice_param</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">gathered_result</span></div>


<span class="n">TReturn</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;TReturn&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="rank_zero_fn"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.rank_zero_fn.html#torchtnt.utils.distributed.rank_zero_fn">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">rank_zero_fn</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function that can be used as a decorator to enable a function to be called on global rank 0 only.</span>

<span class="sd">    Note:</span>
<span class="sd">        This decorator should be used judiciously. it should never be used on functions that need synchronization.</span>
<span class="sd">        It should be used very carefully with functions that mutate local state as well</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; from torchtnt.utilities.distributed import rank_zero_fn</span>
<span class="sd">        &gt;&gt;&gt; @rank_zero_fn</span>
<span class="sd">        ... def foo():</span>
<span class="sd">        ...     return 1</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = foo() # x is 1 if global rank is 0 else x is None</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the desired function to be executed on rank 0 only</span>

<span class="sd">    Return:</span>
<span class="sd">        wrapped_fn: the wrapped function that executes only if the global rank is  0</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">TParams</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">TParams</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">get_global_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">wrapped_fn</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">_BatchNormXd</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">_BatchNorm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The only difference between :class:`torch.nn.BatchNorm1d`, :class:`torch.nn.BatchNorm2d`,</span>
<span class="sd">    :class:`torch.nn.BatchNorm3d`, etc is this method that is overwritten by the sub-class.</span>
<span class="sd">    This method is used when calling forward as a sanity check.</span>
<span class="sd">    When using :function:`revert_sync_batchnorm` this sanity check is lost.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_input_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>


<div class="viewcode-block" id="revert_sync_batchnorm"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.revert_sync_batchnorm.html#torchtnt.utils.distributed.revert_sync_batchnorm">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">revert_sync_batchnorm</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to convert all :class:`torch.nn.SyncBatchNorm` layers in the module to</span>
<span class="sd">    :attr:`BatchNorm*D` layers. This function reverts :meth:`torch.nn.SyncBatchNorm.convert_sync_batchnorm`.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module containing one or more :class:`torch.nn.SyncBatchNorm` layers</span>
<span class="sd">        device (optional): device in which the :attr:`BatchNorm*D` should be created,</span>
<span class="sd">                default is cpu</span>

<span class="sd">    Returns:</span>
<span class="sd">        The original :attr:`module` with the converted :attr:`BatchNorm*D`</span>
<span class="sd">        layers. If the original :attr:`module` is a :class:`torch.nn.SyncBatchNorm` layer,</span>
<span class="sd">        a new :attr:`BatchNorm*D` layer object will be returned</span>
<span class="sd">        instead. Note that the :attr:`BatchNorm*D` layers returned will not have input dimension information.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # Network with nn.BatchNorm layer</span>
<span class="sd">        &gt;&gt;&gt; module = torch.nn.Sequential(</span>
<span class="sd">        &gt;&gt;&gt;            torch.nn.Linear(20, 100),</span>
<span class="sd">        &gt;&gt;&gt;            torch.nn.BatchNorm1d(100),</span>
<span class="sd">        &gt;&gt;&gt;          ).cuda()</span>
<span class="sd">        &gt;&gt;&gt; sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module)</span>
<span class="sd">        &gt;&gt;&gt; reverted_module = revert_sync_batchnorm(sync_bn_module, torch.device(&quot;cuda&quot;))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_output</span> <span class="o">=</span> <span class="n">module</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">):</span>
        <span class="n">module_output</span> <span class="o">=</span> <span class="n">_BatchNormXd</span><span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">track_running_stats</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">module_output</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
                <span class="n">module_output</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">running_mean</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">num_batches_tracked</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_batches_tracked</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;qconfig&quot;</span><span class="p">):</span>
            <span class="c1"># pyre-fixme[16]: `_BatchNormXd` has no attribute `qconfig`.</span>
            <span class="n">module_output</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">qconfig</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">revert_sync_batchnorm</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">device</span><span class="p">))</span>
    <span class="k">del</span> <span class="n">module</span>
    <span class="k">return</span> <span class="n">module_output</span></div>


<div class="viewcode-block" id="sync_bool"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.sync_bool.html#torchtnt.utils.distributed.sync_bool">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">sync_bool</span><span class="p">(</span>
    <span class="n">val</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">coherence_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;any&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;rank_zero&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;any&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility to synchronize a boolean value across members of a provided process group.</span>

<span class="sd">    In the case ``torch.distributed`` is not available or initialized, the input ``val`` is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        val (bool): boolean value to synchronize</span>
<span class="sd">        pg: process group to use for synchronization. If not specified, the default process group is used.</span>
<span class="sd">        coherence_mode Union[str, int, float]: the manner in which the boolean value should be synchronized. 5 options are currently supported:</span>
<span class="sd">            1. any (default): If any rank provides a True value, all ranks should receive True.</span>
<span class="sd">            2. all: Only if all ranks provide a True value should all ranks receive True.</span>
<span class="sd">            3. rank_zero: Makes rank 0 process&#39;s value the source of truth and broadcasts the result to all other processes.</span>
<span class="sd">            4. If an integer N is provided, return True only if at least N processes provide a True value.</span>
<span class="sd">            5. If a float F is provided, return True only if at least this ratio of processes provide a True value. The ratio provided should be in the range [0, 1].</span>

<span class="sd">    Returns:</span>
<span class="sd">        The synchronized boolean value.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; val = True</span>
<span class="sd">        &gt;&gt;&gt; # synced_val is True iff all ranks provide a True value to the function</span>
<span class="sd">        &gt;&gt;&gt; synced_val = sync_bool(val, coherence_mode=&quot;all&quot;)</span>
<span class="sd">        &gt;&gt;&gt; if synced_val:</span>
<span class="sd">        &gt;&gt;&gt;     print(&quot;success&quot;)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">val</span>

    <span class="n">pg</span> <span class="o">=</span> <span class="n">pg</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="p">)</span>
    <span class="n">pg_wrapper</span> <span class="o">=</span> <span class="n">PGWrapper</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">if</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">256</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span>

    <span class="n">indicator</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">val</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;rank_zero&quot;</span><span class="p">:</span>
        <span class="c1"># Broadcast from rank 0 to all other ranks</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">pg</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">indicator</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;any&quot;</span><span class="p">:</span>
        <span class="c1"># sum up the indicators across all the ranks.</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coherence_mode</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># if &gt;= int(coherence_mode) processes signal to stop, all processes stop</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">coherence_mode</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coherence_mode</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span> <span class="o">&gt;=</span> <span class="n">coherence_mode</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;Invalid value for `coherence_mode` provided: Expected type int, float, or one of (&quot;any&quot;, &quot;all&quot;, &quot;rank_zero&quot;), but received </span><span class="si">{</span><span class="n">coherence_mode</span><span class="si">}</span><span class="s1">.&#39;</span>
        <span class="p">)</span></div>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ProcessGroupSetupParams</span><span class="p">:</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">port</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">timeout_s</span><span class="p">:</span> <span class="nb">int</span>


<div class="viewcode-block" id="spawn_multi_process"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.spawn_multi_process.html#torchtnt.utils.distributed.spawn_multi_process">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">spawn_multi_process</span><span class="p">(</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">],</span>
    <span class="o">*</span><span class="n">method_args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="o">**</span><span class="n">method_kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Spawn single node, multi-rank function.</span>
<span class="sd">    Uses localhost and free port to communicate.</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: number of processes</span>
<span class="sd">        backend: backend to use. for example, &quot;nccl&quot;, &quot;gloo&quot;, etc</span>
<span class="sd">        method: callable to spawn.</span>
<span class="sd">        method_args: args for the method</span>
<span class="sd">        method_kwargs: kwargs for the method</span>

<span class="sd">    Note:</span>
<span class="sd">        The default timeout used for distributed collectives in the process group is 60 seconds.</span>
<span class="sd">        This can be overridden by passing a `timeout_s` key in the `method_kwargs`. It will be</span>
<span class="sd">        extracted before passing to the method call.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list, l, where l[i] is the return value of method(*method_args, **methods_kwargs) on rank i</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Manager</span><span class="p">()</span>
    <span class="n">mp_output_dict</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>

    <span class="n">port</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_free_port</span><span class="p">())</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
        <span class="c1"># torch.multiprocessing.spawn sends rank as the first param</span>
        <span class="c1"># https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn</span>
        <span class="n">_init_pg_and_rank_and_launch_method</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span>
            <span class="n">ProcessGroupSetupParams</span><span class="p">(</span>
                <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                <span class="n">port</span><span class="o">=</span><span class="n">port</span><span class="p">,</span>
                <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                <span class="n">timeout_s</span><span class="o">=</span><span class="n">method_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;timeout_s&quot;</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span>
            <span class="p">),</span>
            <span class="n">mp_output_dict</span><span class="p">,</span>
            <span class="n">method</span><span class="p">,</span>
            <span class="n">method_args</span><span class="p">,</span>
            <span class="n">method_kwargs</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">output_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
        <span class="n">output_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mp_output_dict</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">output_list</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_init_pg_and_rank_and_launch_method</span><span class="p">(</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">pg_setup_params</span><span class="p">:</span> <span class="n">ProcessGroupSetupParams</span><span class="p">,</span>
    <span class="n">mp_output_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">],</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg_setup_params</span><span class="o">.</span><span class="n">port</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
        <span class="n">world_size</span><span class="o">=</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span>  <span class="c1"># setting up timeout for distributed collectives</span>
            <span class="n">seconds</span><span class="o">=</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">timeout_s</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># pyre-ignore: spawn_multi_process uses unsafe types to begin with</span>
        <span class="n">mp_output_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">method</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">destroy_process_group</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">rank_zero_read_and_broadcast</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decorator that ensures a function is only executed by rank 0 and returns the result to all ranks.</span>

<span class="sd">    Note:</span>
<span class="sd">        By default will use the global process group. To use a custom process group, `process_group` must be an arg to the function and passed as a keyword argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">TParams</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">TParams</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TReturn</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">get_global_rank</span><span class="p">()</span>
        <span class="n">process_group</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;process_group&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># pyre-ignore[16]</span>

        <span class="c1"># Do all filesystem reads from rank 0 only</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># If not running in a distributed setting, return as is</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()):</span>
            <span class="c1"># we cast here to avoid type errors, since it is</span>
            <span class="c1"># guaranteed the return value is of type T</span>
            <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">TReturn</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>

        <span class="c1"># Otherwise, broadcast result from rank 0 to all ranks</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">PGWrapper</span><span class="p">(</span><span class="n">process_group</span><span class="p">)</span>
        <span class="n">path_container</span> <span class="o">=</span> <span class="p">[</span><span class="n">ret</span><span class="p">]</span>
        <span class="n">pg</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">path_container</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">path_container</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># we cast here to avoid type errors, since it is</span>
        <span class="c1"># guaranteed the return value is of type T</span>
        <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">TReturn</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapper</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_or_create_gloo_pg</span><span class="p">(</span>
    <span class="n">candidate_pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Context manager to ensure that a gloo process group is used for the contained operations. First checks if the</span>
<span class="sd">    WORLD process group, or the provided candidate process group, is already gloo-based. In case it is, that is returned.</span>
<span class="sd">    Otherwise, a new gloo process group will be created and returned. Upon exiting the context, if a new process group</span>
<span class="sd">    was created, it will be destroyed.</span>

<span class="sd">    Note: If the distributed environment is not initialized, this context manager will return None and will be no-op.</span>

<span class="sd">    Args:</span>
<span class="sd">        candidate_pg: Optional process group to check if it is gloo-based. If None, the WORLD process group will be checked.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gloo_pg_created</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Not in a distributed environment, gloo process group not created&quot;</span><span class="p">)</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">candidate_pg</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Creating temporary gloo process group&quot;</span><span class="p">)</span>
            <span class="n">pg</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">3600</span><span class="p">),</span> <span class="n">backend</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span>
            <span class="p">)</span>
            <span class="n">gloo_pg_created</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">pg</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="c1"># Cleanup temporary gloo pg if it was created</span>
        <span class="k">if</span> <span class="n">gloo_pg_created</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Destroyed temporary gloo process group&quot;</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/js/torchtnt.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>